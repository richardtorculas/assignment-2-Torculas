{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing MPI Library**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nwBreKv3xJMZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tyi_mZmiuKgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5f87d2f-5fa0-430c-e1c3-321e56ec1fbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.12/dist-packages (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install mpi4py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating the MPI Python File using the** ***STARTER CODE***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WmOptxdfxVh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_pdc.py\n",
        "from mpi4py import MPI\n",
        "import sys\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "if rank == 0:\n",
        "    for i in range(1, size):\n",
        "        message = comm.recv(source=i)\n",
        "        print(f\"Received from process {i}: {message}\")\n",
        "else:\n",
        "  comm.send(f\"Hello from process {rank}\", dest=0)"
      ],
      "metadata": {
        "id": "bvJEGocTuYdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf0f008-944d-4d71-e172-1aca01b639f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi_pdc.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Root Access Configuration**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Y6_cHCAmxoAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OMPI_ALLOW_RUN_AS_ROOT\"] = \"1\"\n",
        "os.environ[\"OMPI_ALLOW_RUN_AS_ROOT_CONFIRM\"] = \"1\"\n"
      ],
      "metadata": {
        "id": "jpaFdvQ7wMcE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Executing the Distributed Program**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kEJZP_Dox0q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mpiexec --oversubscribe -n 4 python mpi_pdc.py"
      ],
      "metadata": {
        "id": "oeIf1RV3vODU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb31646-ecd6-4d74-ec25-fe0ac309d9a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Received from process 1: Hello from process 1\n",
            "Received from process 2: Hello from process 2\n",
            "Received from process 3: Hello from process 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Advanced Computation Assignment: Implementing Distributed Workloads**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-yAH8avCySDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_pdc.py\n",
        "from mpi4py import MPI\n",
        "import sys\n",
        "\n",
        "# Initialize MPI communication world\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "# List of operations to assign to processes\n",
        "operations = [\"+\", \"-\", \"/\", \"*\"]\n",
        "\n",
        "if rank == 0:\n",
        "    # --- MASTER PROCESS LOGIC ---\n",
        "    print(f\"\\n{' PROCESS ID ':#^20} | {' ASSIGNED TASK ':#^20} | {' RESULT ':#^15}\")\n",
        "    print(f\"{'':-^61}\")\n",
        "\n",
        "    for i in range(1, size):\n",
        "        # Receive structured data dictionary from workers\n",
        "        data = comm.recv(source=i)\n",
        "\n",
        "        p_id = f\"Worker {data['rank']}\"\n",
        "        task = data['task']\n",
        "        # Format floats to 2 decimal places, keep integers as is\n",
        "        res = f\"{data['result']:.2f}\" if isinstance(data['result'], float) else data['result']\n",
        "\n",
        "        print(f\"{p_id:<20} | {task:^20} | {res:>15}\")\n",
        "\n",
        "    print(f\"{'':-^61}\")\n",
        "    print(f\"Master: Successfully collected results from {size-1} workers.\\n\")\n",
        "\n",
        "else:\n",
        "    # --- WORKER PROCESS LOGIC ---\n",
        "    # Assign an operator based on rank\n",
        "    op = operations[rank % len(operations)]\n",
        "    val_a = rank * 10\n",
        "    val_b = 2\n",
        "\n",
        "    # Perform simple computation\n",
        "    if op == \"+\": result = val_a + val_b\n",
        "    elif op == \"-\": result = val_a - val_b\n",
        "    elif op == \"/\": result = val_a / val_b\n",
        "    else: result = val_a * val_b\n",
        "\n",
        "    # Send result package back to Master\n",
        "    payload = {\n",
        "        \"rank\": rank,\n",
        "        \"task\": f\"{val_a} {op} {val_b}\",\n",
        "        \"result\": result\n",
        "    }\n",
        "    comm.send(payload, dest=0)"
      ],
      "metadata": {
        "id": "tm9jlSdtx6Gl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "781d9346-47f9-487a-c6fc-59dc255a12ae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi_pdc.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpiexec --oversubscribe -n 10 python mpi_pdc.py"
      ],
      "metadata": {
        "id": "VYiIlGzcvWYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71c6d393-ddbe-43fd-d985-a8ef1f68f66e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "#### PROCESS ID #### | ## ASSIGNED TASK ### | ### RESULT ####\n",
            "-------------------------------------------------------------\n",
            "Worker 1             |        10 - 2        |               8\n",
            "Worker 2             |        20 / 2        |           10.00\n",
            "Worker 3             |        30 * 2        |              60\n",
            "Worker 4             |        40 + 2        |              42\n",
            "Worker 5             |        50 - 2        |              48\n",
            "Worker 6             |        60 / 2        |           30.00\n",
            "Worker 7             |        70 * 2        |             140\n",
            "Worker 8             |        80 + 2        |              82\n",
            "Worker 9             |        90 - 2        |              88\n",
            "-------------------------------------------------------------\n",
            "Master: Successfully collected results from 9 workers.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Follow Up Questions:**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "H2luRANJ0f8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Why is message passing required in distributed systems?\n",
        "- In a distributed environment, processes are fundamentally isolated; each operates within its own private memory address space. Because one process cannot \"reach into\" the RAM of another to read or write data, an explicit mechanism is required to bridge this gap.\n",
        "###What happens if one process fails?\n",
        "- Without built-in fault tolerance, a basic MPI program will hang indefinitely if a worker process fails before sending its result. This reveals a critical vulnerability where the master remains in a blocking wait, demonstrating that a single point of failure can paralyze the entire distributed computation.\n",
        "###How does this model differ from shared-memory programming?\n",
        "- Message passing requires explicit communication between private memory spaces, which increases coding complexity but naturally prevents data races and allows for massive scaling across clusters. In contrast, shared-memory programming allows threads to access a common memory pool directly, which is simpler for local tasks but requires strict synchronization to avoid data conflicts.\n",
        "\n",
        "**SUMMARY**\n",
        "- In distributed systems, message passing is essential because processes operate in isolated memory spaces and must explicitly exchange data to coordinate tasks. This model is highly scalable and prevents data races, but it lacks inherent fault tolerance, meaning a single process failure can cause the entire system to hang. Ultimately, while it is more complex to implement than shared-memory programming, message passing provides a safer and more robust framework for computing across multiple machines."
      ],
      "metadata": {
        "id": "RCGEYMjQ0H19"
      }
    }
  ]
}